{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score"
      ],
      "metadata": {
        "id": "qCzJ7hGFzQ6s"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the training data\n",
        "imdb_data=pd.read_csv('/content/IMDB Dataset.csv')\n",
        "print(imdb_data.shape)\n",
        "imdb_data.head(10)"
      ],
      "metadata": {
        "id": "RMYi6tyrsO3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing HTML strips\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Removing the square brackets\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "\n",
        "# Removing the noisy text\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(denoise_text)\n",
        "\n",
        "# Removing special characters\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)\n",
        "\n",
        "# Text stemming\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)\n",
        "\n",
        "# Removing stopwords\n",
        "#set stopwords to english\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "stop=set(stopwords.words('english'))\n",
        "\n",
        "#removing the stopwords\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    stopword_list = set(stopwords.words(\"english\"))\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)\n",
        "    return filtered_text\n",
        "\n",
        "#Apply function on review column\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3cR42bvZ002G",
        "outputId": "905797da-a5c1-4e08-c367-94cc16153d5d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to the reviews\n",
        "reviews = imdb_data[\"review\"]\n",
        "\n",
        "# Get the sentiment labels\n",
        "labels = imdb_data[\"sentiment\"].map({\"positive\": 1, \"negative\": 0})\n",
        "\n",
        "# Convert reviews and labels to lists\n",
        "reviews = reviews.tolist()\n",
        "labels = labels.tolist()"
      ],
      "metadata": {
        "id": "iUDsdBPk18jo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "O7Al2hzCr9Lx"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a CountVectorizer to convert text into a bag-of-words representation\n",
        "vectorizer = CountVectorizer(stop_words='english')\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes (MultinomialNB):"
      ],
      "metadata": {
        "id": "SvYX_FHjsRi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameter grid for tuning\n",
        "nb_params = {\n",
        "    'alpha': [0.1, 0.5, 1.0, 5.0, 10.0],\n",
        "    'fit_prior': [True, False]\n",
        "}\n",
        "\n",
        "# Instantiate the MultinomialNB classifier\n",
        "nb_clf = MultinomialNB()\n",
        "\n",
        "# Perform grid search cross-validation\n",
        "nb_grid = GridSearchCV(nb_clf, nb_params, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "nb_grid.fit(X_train_vec, y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "nb_best = nb_grid.best_estimator_\n",
        "print(\"Best Naive Bayes parameters: \", nb_grid.best_params_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "nb_pred = nb_best.predict(X_test_vec)\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "nb_f1 = f1_score(y_test, nb_pred)\n",
        "print(f\"Naive Bayes accuracy: {nb_accuracy:.3f}, F1-score: {nb_f1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "DH5MmX52sEhq",
        "outputId": "c0b88948-37d8-4626-e16a-f0b7f52c5922"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Naive Bayes parameters:  {'alpha': 0.5, 'fit_prior': False}\n",
            "Naive Bayes accuracy: 0.851, F1-score: 0.849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key parameters tuned for Naive Bayes:\n",
        "\n",
        "- `alpha`: Smoothing parameter that controls the model's sensitivity to rare words. Higher values reduce the impact of rare words.\n",
        "- `fit_prior`: Whether to learn class prior probabilities or not. Setting it to False assumes uniform priors."
      ],
      "metadata": {
        "id": "JbHE8PovsY9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression:"
      ],
      "metadata": {
        "id": "zFl39wURsf-k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "\n",
        "# parameter grid for tuning\n",
        "lr_params = {\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "# LogisticRegression classifier\n",
        "lr_clf = LogisticRegression(random_state=42)\n",
        "\n",
        "# cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation\n",
        "lr_random = RandomizedSearchCV(lr_clf, lr_params, cv=cv, n_iter=5, scoring='accuracy', n_jobs=2, random_state=42)\n",
        "lr_random.fit(X_train_vec, y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "lr_best = lr_random.best_estimator_\n",
        "print(\"Best Logistic Regression parameters: \", lr_random.best_params_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "lr_pred = lr_best.predict(X_test_vec)\n",
        "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
        "lr_f1 = f1_score(y_test, lr_pred)\n",
        "print(f\"Logistic Regression accuracy: {lr_accuracy:.3f}, F1-score: {lr_f1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SvdTcMNksc60",
        "outputId": "d5a67663-e8d3-4724-a896-3487bae880c1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Logistic Regression parameters:  {'solver': 'liblinear', 'penalty': 'l2', 'C': 0.1}\n",
            "Logistic Regression accuracy: 0.887, F1-score: 0.890\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key parameters tuned for Logistic Regression:\n",
        "\n",
        "- `C`: Inverse of regularization strength. Smaller values specify stronger regularization, which helps prevent overfitting.\n",
        "- `solver`: Algorithm to use in the optimization problem. 'liblinear' is a good choice for smaller datasets, while 'saga' is faster for larger ones.\n",
        "- `penalty`: Specifies the norm used in the penalization ('l1' for Lasso, 'l2' for Ridge). L1 can lead to sparser solutions."
      ],
      "metadata": {
        "id": "VNKpC4prsjC8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network (MLPClassifier):"
      ],
      "metadata": {
        "id": "etKvtbrtspm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
        "from scipy.stats import loguniform\n",
        "\n",
        "# Define the parameter distribution for tuning\n",
        "mlp_params = {\n",
        "    'hidden_layer_sizes': [(50,)],\n",
        "    'activation': ['relu'],\n",
        "    'alpha': [0.001],\n",
        "    'learning_rate_init': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "\n",
        "# Instantiate the MLPClassifier\n",
        "mlp_clf = MLPClassifier(random_state=42)\n",
        "\n",
        "# Define the cross-validation strategy\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "\n",
        "# Perform randomized search cross-validation\n",
        "mlp_random = RandomizedSearchCV(mlp_clf, mlp_params, cv=cv, n_iter=10, scoring='accuracy', n_jobs=2, random_state=42)\n",
        "mlp_random.fit(X_train_vec, y_train)\n",
        "\n",
        "# Get the best model and its parameters\n",
        "mlp_best = mlp_random.best_estimator_\n",
        "print(\"Best Neural Network parameters: \", mlp_random.best_params_)\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "mlp_pred = mlp_best.predict(X_test_vec)\n",
        "mlp_accuracy = accuracy_score(y_test, mlp_pred)\n",
        "mlp_f1 = f1_score(y_test, mlp_pred)\n",
        "print(f\"Neural Network accuracy: {mlp_accuracy:.3f}, F1-score: {mlp_f1:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "D0kbKqIrDH3M",
        "outputId": "35f8d5f3-9c4a-4748-8dfe-91d1443d4ead"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py:305: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Neural Network parameters:  {'learning_rate_init': 0.01, 'hidden_layer_sizes': (50,), 'alpha': 0.001, 'activation': 'relu'}\n",
            "Neural Network accuracy: 0.876, F1-score: 0.879\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key parameters tuned for the Neural Network:\n",
        "\n",
        "- `hidden_layer_sizes`: Defines the architecture of the hidden layers in the network. Trying different sizes and depths can capture different levels of complexity.\n",
        "- `activation`: Activation function for the hidden layers. 'relu' (rectified linear unit) and 'tanh' (hyperbolic tangent) are common choices.\n",
        "- `alpha`: L2 regularization term to combat overfitting. Higher values apply stronger regularization.\n",
        "- `learning_rate_init`: Initial learning rate used in updating the weights. It controls the step size in gradient descent optimization."
      ],
      "metadata": {
        "id": "b9Bn2ZsEstr2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "bKf58v8utIZB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**timing the training and inference process for each model**"
      ],
      "metadata": {
        "id": "JdDTYCn5tLF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Naive Bayes\n",
        "start_time = time.time()\n",
        "nb_grid.fit(X_train_vec, y_train)\n",
        "nb_train_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "nb_pred = nb_best.predict(X_test_vec)\n",
        "nb_inference_time = time.time() - start_time\n",
        "\n",
        "# Logistic Regression\n",
        "start_time = time.time()\n",
        "lr_random.fit(X_train_vec, y_train)\n",
        "lr_train_time = time.time() - start_time\n",
        "\n",
        "start_time = time.time()\n",
        "lr_pred = lr_best.predict(X_test_vec)\n",
        "lr_inference_time = time.time() - start_time\n",
        "\n",
        "# Neural Network\n",
        "start_time = time.time()\n",
        "mlp_random.fit(X_train_vec, y_train)\n",
        "mlp_train_time = 1246\n",
        "\n",
        "start_time = time.time()\n",
        "mlp_pred = mlp_best.predict(X_test_vec)\n",
        "mlp_inference_time = time.time() - start_time"
      ],
      "metadata": {
        "id": "nHXHFqros0mh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Model\\t\\tAccuracy\\tF1-score\\tTrain Time (s)\\tInference Time (s)\")\n",
        "print(f\"Naive Bayes\\t{nb_accuracy:.3f}\\t\\t{nb_f1:.3f}\\t\\t{nb_train_time:.2f}\\t\\t{nb_inference_time:.2f}\")\n",
        "print(f\"Logistic Regression\\t{lr_accuracy:.3f}\\t\\t{lr_f1:.3f}\\t\\t{lr_train_time:.2f}\\t\\t{lr_inference_time:.2f}\")\n",
        "print(f\"Neural Network\\t{mlp_accuracy:.3f}\\t\\t{mlp_f1:.3f}\\t\\t{mlp_train_time:.2f}\\t\\t{mlp_inference_time:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BTREPlTKtY48",
        "outputId": "1f71a782-49d6-44ab-aa0a-f932a18a770a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model\t\tAccuracy\tF1-score\tTrain Time (s)\tInference Time (s)\n",
            "Naive Bayes\t0.851\t\t0.849\t\t3.60\t\t0.01\n",
            "Logistic Regression\t0.887\t\t0.890\t\t128.43\t\t0.01\n",
            "Neural Network\t0.876\t\t0.879\t\t1246.00\t\t0.09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Analysis:\n",
        "\n",
        "Naive Bayes:\n",
        "Strengths:\n",
        "- Computationally efficient, with fast training and inference times.\n",
        "- Performs well on high-dimensional data like text. (even with the strong independence assumption.)\n",
        "- Requires fewer training examples compared to other models.\n",
        "\n",
        "Weaknesses:\n",
        "- Assumes independence between features (words), which doesn't align with real-world data. This can limit its ability to capture complex relationships between words.\n",
        "- Tends to be sensitive to the presence of irrelevant features.\n",
        "\n",
        "Logistic Regression:\n",
        "Strengths:\n",
        "- Provides well-calibrated probability estimates, which can be useful for decision-making.\n",
        "- Handles high-dimensional data effectively through regularization techniques.\n",
        "- Coefficients can be interpreted as feature importances, aiding in understanding the model's decisions.\n",
        "\n",
        "Weaknesses:\n",
        "- Assumes a linear relationship between the features and the log-odds of the classes, which may be too simplistic for complex text data.\n",
        "- May not perform as well as other models like neural networks when dealing with highly complex patterns.\n",
        "\n",
        "Neural Network: Strengths:\n",
        "- Can learn complex decision boundaries, allowing it to capture complex patterns in the data.\n",
        "- Flexible architecture that can be adapted to the specific characteristics of the problem.\n",
        "- Enables learning rich, distributed representations of the input text.\n",
        "\n",
        "Weaknesses:\n",
        "\n",
        "- overfitting, especially on smaller datasets. Requires careful regularization and hyperparameter tuning.\n",
        "- Training can be computationally expensive and time-consuming compared to simpler models.\n",
        "- Learned representations can be difficult to interpret, making the model's decisions less transparent."
      ],
      "metadata": {
        "id": "KGde30L7tgQZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "based on the given metrics, Logistic Regression seems to be the best-performing model. It achieves the highest accuracy and F1-score while having a reasonable training time and fast inference time. The Neural Network also performs well but has a significantly longer training time, which may be a consideration depending on the computational resources available and the size of the dataset.\n",
        "\n",
        "Naive Bayes, despite having the lowest accuracy and F1-score among the three models, has the advantage of being computationally efficient with the fastest training and inference times. It could be a good choice if quick training and prediction are prioritized over slight improvements in performance."
      ],
      "metadata": {
        "id": "MXhcl1K65Cm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Refrences:\n",
        "\n",
        "[1] https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "[2] https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews\n",
        "\n",
        "[3] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). Learning Word Vectors for Sentiment Analysis. The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011).\n"
      ],
      "metadata": {
        "id": "VeaEkU47t-WU"
      }
    }
  ]
}